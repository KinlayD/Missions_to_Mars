{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# define path to chrome driver\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open browser for splinter scraping\n",
    "\n",
    "# create a new browser object \n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NASA MARS NEWS\n",
    "    # Scrape the Mars News Site and collect the latest News Title and Paragraph Text. \n",
    "    # Assign the text to variables that you can reference later.\n",
    "\n",
    "# visit url:\n",
    "url = 'https://redplanetscience.com/'\n",
    "browser.visit(url)\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "# scrape news title and paragraph text from soup object\n",
    "\n",
    "for x in range(1):\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    image_and_descriptions = soup.find_all('section', class_='image_and_description_container')\n",
    "    for text in image_and_descriptions:\n",
    "        content_title_tag = text.find(class_=\"content_title\").text\n",
    "        news_title = content_title_tag\n",
    "        news_p = text.find(class_=\"article_teaser_body\").text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JPL MARS SPACE IMAGES - FEATURED IMAGE\n",
    "    # Use splinter to navigate the site and find the image url for the current Featured Mars Image and \n",
    "        # assign the url string to a variable called featured_image_url.\n",
    "    # Make sure to find the image url to the full size .jpg image.\n",
    "    # Make sure to save a complete url string for this image.\n",
    "\n",
    "# visit url\n",
    "url_1 = 'https://spaceimages-mars.com/'\n",
    "browser.visit(url_1)\n",
    "time.sleep(1)\n",
    "\n",
    "# scrape img url and concatinate with link\n",
    "for x in range(1):\n",
    "    html_1 = browser.html\n",
    "    soup_1 = BeautifulSoup(html_1, 'html.parser')\n",
    "    header_html = soup_1.find_all('a', class_=\"showimg fancybox-thumbs\")\n",
    "    for link in header_html:\n",
    "        img_tag = link['href']\n",
    "        full_url = url_1 + img_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MARS HEMISPHERES\n",
    "    # Visit the Astrogeology site here to obtain high resolution images for each of Mars' hemispheres.\n",
    "    # You will need to click each of the links to the hemispheres in order to find the image url to the \n",
    "        # full resolution image.\n",
    "    # Save both the image url string for the full resolution hemisphere image, and the Hemisphere title \n",
    "        # containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "    # Append the dictionary with the image url string and the hemisphere title to a list. \n",
    "        # This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "# visit url\n",
    "url_2 = 'https://marshemispheres.com/'\n",
    "browser.visit(url_2)\n",
    "time.sleep(1)\n",
    "html_2 = browser.html\n",
    "soup_2 = BeautifulSoup(html_2, 'html.parser')\n",
    "\n",
    "# scrape title:\n",
    "full_content_html = soup_2.find('div', class_='full-content')\n",
    "div_tag = full_content_html.find_all('div', class_='description')\n",
    "title =[]\n",
    "for h3 in div_tag:\n",
    "    tag_text = h3.find('h3').text\n",
    "    title.append(tag_text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit url\n",
    "url_2 = 'https://marshemispheres.com/'\n",
    "browser.visit(url_2)\n",
    "time.sleep(1)\n",
    "html_2 = browser.html\n",
    "soup_2 = BeautifulSoup(html_2, 'html.parser')\n",
    "\n",
    "# scrape title and href:\n",
    "item_html = soup_2.find_all('div', class_='item')\n",
    "title =[]\n",
    "href = []\n",
    "for h3 in item_html:\n",
    "    tag_text = h3.find('h3').text\n",
    "    title.append(tag_text)\n",
    "    a_tag = h3.find('a')['href']\n",
    "    href.append(a_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2 = 'https://marshemispheres.com/'\n",
    "browser.visit(url_2)\n",
    "time.sleep(1)\n",
    "\n",
    "downloaded_url = []\n",
    "image_url =[]\n",
    "src = []\n",
    "for t in title:\n",
    "    browser.find_by_tag('h3').first.click()\n",
    "    html_3 = browser.html\n",
    "    soup_3 = BeautifulSoup(html_3, 'html.parser')\n",
    "    time.sleep(1)\n",
    "    downloads = soup_3.find('img')['src']\n",
    "    downloaded_url.append(downloads)\n",
    "    browser.find_by_tag('h3').last.click()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images/usgs_logo_main_2x.png',\n",
       " 'images/usgs_logo_main_2x.png',\n",
       " 'images/usgs_logo_main_2x.png',\n",
       " 'images/usgs_logo_main_2x.png']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = downloads['src']\n",
    "downloads = soup_3.find('img', class_='wide-image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_2 = 'https://marshemispheres.com/'\n",
    "browser.visit(url_2)\n",
    "time.sleep(1)\n",
    "\n",
    "downloaded_url = []\n",
    "image_url =[]\n",
    "for t in title:\n",
    "    browser.find_by_tag('h3').first.click()\n",
    "    html_3 = browser.html\n",
    "    soup_3 = BeautifulSoup(html_3, 'html.parser')\n",
    "    time.sleep(1)\n",
    "    downloads = soup_3.find('div', class_='downloads')\n",
    "    downloaded_url.append(downloads)\n",
    "    browser.find_by_tag('h3').last.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"downloads\">\n",
       " <img class=\"thumb\" src=\"images/39d3266553462198bd2fbc4d18fbed17_cerberus_enhanced.tif_thumb.png\"/>\n",
       " <h3>Download</h3>\n",
       " <ul>\n",
       " <li><a href=\"images/full.jpg\" target=\"_blank\">Sample</a> (jpg) 1024px wide</li>\n",
       " <li><a href=\"images/cerberus_enhanced.tif\" target=\"_blank\">Original</a> (tif<span class=\"tooltip word-tif\" title=\"\"></span>) 21 MB</li>\n",
       " </ul>\n",
       " </div>,\n",
       " <div class=\"downloads\">\n",
       " <img class=\"thumb\" src=\"images/39d3266553462198bd2fbc4d18fbed17_cerberus_enhanced.tif_thumb.png\"/>\n",
       " <h3>Download</h3>\n",
       " <ul>\n",
       " <li><a href=\"images/full.jpg\" target=\"_blank\">Sample</a> (jpg) 1024px wide</li>\n",
       " <li><a href=\"images/cerberus_enhanced.tif\" target=\"_blank\">Original</a> (tif<span class=\"tooltip word-tif\" title=\"\"></span>) 21 MB</li>\n",
       " </ul>\n",
       " </div>,\n",
       " <div class=\"downloads\">\n",
       " <img class=\"thumb\" src=\"images/39d3266553462198bd2fbc4d18fbed17_cerberus_enhanced.tif_thumb.png\"/>\n",
       " <h3>Download</h3>\n",
       " <ul>\n",
       " <li><a href=\"images/full.jpg\" target=\"_blank\">Sample</a> (jpg) 1024px wide</li>\n",
       " <li><a href=\"images/cerberus_enhanced.tif\" target=\"_blank\">Original</a> (tif<span class=\"tooltip word-tif\" title=\"\"></span>) 21 MB</li>\n",
       " </ul>\n",
       " </div>,\n",
       " <div class=\"downloads\">\n",
       " <img class=\"thumb\" src=\"images/39d3266553462198bd2fbc4d18fbed17_cerberus_enhanced.tif_thumb.png\"/>\n",
       " <h3>Download</h3>\n",
       " <ul>\n",
       " <li><a href=\"images/full.jpg\" target=\"_blank\">Sample</a> (jpg) 1024px wide</li>\n",
       " <li><a href=\"images/cerberus_enhanced.tif\" target=\"_blank\">Original</a> (tif<span class=\"tooltip word-tif\" title=\"\"></span>) 21 MB</li>\n",
       " </ul>\n",
       " </div>]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ElementList' object has no attribute 'find_by_partial_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/bootcamp/lib/python3.7/site-packages/splinter/element_list.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WebDriverElement' object has no attribute 'find_by_partial_text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/bootcamp/lib/python3.7/site-packages/splinter/element_list.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'find_by_partial_text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4l/v56_76l56_q2mt8lngxzjnvw0000gn/T/ipykernel_41531/224699163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_by_partial_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cereberus Hemisphere Enhanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/bootcamp/lib/python3.7/site-packages/splinter/element_list.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 raise AttributeError(\n\u001b[1;32m     84\u001b[0m                     u\"'{0}' object has no attribute '{1}'\".format(\n\u001b[0;32m---> 85\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                     )\n\u001b[1;32m     87\u001b[0m                 )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ElementList' object has no attribute 'find_by_partial_text'"
     ]
    }
   ],
   "source": [
    "a.find_by_partial_text('Cereberus Hemisphere Enhanced').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"description\">\n",
       "<a class=\"itemLink product-item\" href=\"cerberus.html\">\n",
       "<h3>Cerberus Hemisphere Enhanced</h3>\n",
       "</a>\n",
       "<span class=\"subtitle\" style=\"float:left\">image/tiff 21 MB</span><span class=\"pubDate\" style=\"float:right\"></span><br/>\n",
       "<p>Mosaic of the Cerberus hemisphere of Mars projected into point perspective, a view similar to that which one would see from a spacecraft. This mosaic is composed of 104 Viking Orbiter images acquired…</p>\n",
       "</div>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "url_2 = 'https://marshemispheres.com/'\n",
    "browser.visit(url_2)\n",
    "time.sleep(1)\n",
    "\n",
    "hemisphere_titles = []\n",
    "hemisphere_href = []\n",
    "test = soup_2.find('div', class_='description')\n",
    "test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title =[]\n",
    "for h3 in h3_tag:\n",
    "    tag_text = h3.text\n",
    "    title.append(tag_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('bootcamp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efc213f78c2332ade3cb52c89bda8c080bc1ae1c359626bac6e5e7c902af6ee9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
